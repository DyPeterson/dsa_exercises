{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and grouping data:\n",
    "Use Spark in a Python script or Jupyter notebook to answer the following questions. They should combine grouping, filtering and aggregation we learned in the lesson.\n",
    "\n",
    "1. Find the total number, average number, min and max number of `parts`,  in all `sets`.\n",
    "1. Find the total number, average number, min and max number of `parts`,  for each `theme_id` in `sets`.\n",
    "1. Find all `theme_id`s that have more than 200 parts on average.\n",
    "1. Find how many total parts each theme had every year.\n",
    "1. Find the set with the most number of parts each year.\n",
    "1. How many sets do we have with more than one version in `set_inventories`?\n",
    "\n",
    "## UDF\n",
    "To practice Pandas UDFs, create the following functions in a Python script or Jupyter notebook:\n",
    "1. A `pandas_udf` that divides the input Pandas Series by 12 to convert a number into dozens.\n",
    "    1. Apply this to the quantity columns of each inventory CSV.\n",
    "1. A `pandas_udf` that converts strings to upper case.\n",
    "    1. Convert each color name to upper case.\n",
    "    1. Convert each set name to upper case.\n",
    "1. A `pandas_udf` that strips all whitespace from a string.\n",
    "    1. Strip whitespace from theme names.\n",
    "    1. Strip whitespace from color names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find the total number, average number, min and max number of `parts`,  in all `sets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|num_parts|sum(num_parts)|\n",
      "+---------+--------------+\n",
      "|      471|          3768|\n",
      "|      148|          1776|\n",
      "|      496|           992|\n",
      "|      833|          1666|\n",
      "|      463|           463|\n",
      "|     1084|          1084|\n",
      "|      737|           737|\n",
      "|      243|          1944|\n",
      "|      623|          2492|\n",
      "|      392|          1568|\n",
      "|      540|          1080|\n",
      "|      897|          1794|\n",
      "|     1025|          1025|\n",
      "|     1483|          1483|\n",
      "|       31|          2046|\n",
      "|      516|          3612|\n",
      "|     1139|          1139|\n",
      "|     1339|          1339|\n",
      "|     1903|          1903|\n",
      "|      251|          3012|\n",
      "+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sets_df = spark.read.csv('./data/sets.csv', header=True, schema='set_num string,name string,year int,theme_id int,num_parts int')\n",
    "\n",
    "sets_df.groupBy('num_parts').sum('num_parts').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77184e4aa403577b9564228168ef4c84706533db50dba8db351060a7a5eb763d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
